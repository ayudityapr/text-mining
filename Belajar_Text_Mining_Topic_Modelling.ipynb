{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Belajar Text Mining - Topic Modelling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzPMp450RC3y2IzelNDDgW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianz88/text-mining/blob/master/Belajar_Text_Mining_Topic_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9cWSdvzpgYT",
        "colab_type": "text"
      },
      "source": [
        "# Berkenalan dengan Topic Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1VVFCH8pnk1",
        "colab_type": "text"
      },
      "source": [
        "Kita akan belajar untuk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyZ_dJoXpq2N",
        "colab_type": "text"
      },
      "source": [
        "## Persiapan Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh7hGzr4puMl",
        "colab_type": "text"
      },
      "source": [
        "Install beberapa library dan package yang diperlukan dalam project (dijalankan dalam Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_vZw00Jpwdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Library corpus bahasa Indonesia (Sastrawi)\n",
        "!pip install sastrawi\n",
        "\n",
        "# Library Visualisasi\n",
        "!pip install pyldavis\n",
        "\n",
        "# Library machine learning untuk topic modelling\n",
        "!pip install gensim==3.8.0\n",
        "import pkg_resources\n",
        "pkg_resources.get_distribution(\"gensim\").version\n",
        "\n",
        "# Natural Language Tool Kit (NLTK)\n",
        "import nltk\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Python Regex\n",
        "import re\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from gensim import models, corpora\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NQToBQHHRfH",
        "colab_type": "text"
      },
      "source": [
        "## Persiapan Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCG8Y7lGHU8v",
        "colab_type": "text"
      },
      "source": [
        "Fungsi-fungsi yang digunakan untuk mempersiapkan dokumen (teks) yang akan diolah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYn_1APVKDaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fungsi memecah dokumen menjadi token (array elemen per kata)\n",
        "def tokenize_clean(text):\n",
        "    \n",
        "    #tokenisasi\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word\n",
        "        in nltk.word_tokenize(sent)]\n",
        "    \n",
        "    #clean token from numeric and other character like puntuation\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token) and token not in stopwords:\n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    return filtered_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iydwVpvIgyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Daftar Stopwords\n",
        "stopwords_all = nltk.corpus.stopwords.words('indonesian')\n",
        "stopwords_tambahan = {\"ya\",\"yak\",\"iya\",\"yg\",\"ga\",\"gak\",\"gk\",\"udh\",\"sdh\",\"udah\",\"dah\",\"nih\",\"ini\",\"deh\",\"sih\",\"dong\",\"donk\",\n",
        "                 \"sm\",\"knp\",\"utk\",\"yaa\",\"tdk\",\"gini\",\"gitu\",\"bgt\",\"gt\",\"nya\",\"kalo\",\"cb\",\"jg\",\"jgn\",\"gw\",\"ge\",\n",
        "                 \"sy\",\"min\",\"mas\",\"mba\",\"mbak\",\"pak\",\"kak\",\"trus\",\"trs\",\"bs\",\"bisa\",\"aja\",\"saja\",\"no\",\n",
        "                 \"w\",\"g\",\"gua\",\"gue\",\"emang\",\"emg\",\"wkwk\",\"dr\",\"kau\",\"dg\",\"gimana\",\"apapun\",\"apa\",\n",
        "                 \"klo\",\"yah\",\"banget\",\"pake\",\"terus\",\"krn\",\"jadi\",\"jd\",\"mu\",\"ku\",\"si\",\"hehe\",\n",
        "                 \"tp\",\"pa\",\"lu\",\"lo\",\"lw\",\"tw\",\"tau\",\"karna\",\"kayak\",\"ky\",\"lg\",\"untuk\",\"tuk\",\"dg\",\"dgn\"\n",
        "                }\n",
        "stopwords_all.extend(stopwords_tambahan)\n",
        "stopwords = stopwords_all\n",
        "print(len(stopwords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_4IYJgLKGk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fungsi menghilangkan stopwords dan tanda baca\n",
        "def remove_stopwords(tokenized_text):\n",
        "    \n",
        "    cleaned_token = []\n",
        "    for token in tokenized_text:\n",
        "        if token not in stopwords:\n",
        "            cleaned_token.append(token)\n",
        "            \n",
        "    return cleaned_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAhTsQ0yKQtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fungsi mengubah kata ke bentuk kata dasar (bahasa Indonesia)\n",
        "def stemming_text(tokenized_text):\n",
        "    \n",
        "    #stem using Sastrawi StemmerFactory \n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "\n",
        "    stems = []\n",
        "    for token in tokenized_text:\n",
        "        stems.append(stemmer.stem(token))\n",
        "\n",
        "    return stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6q3tC-5Kfz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fungsi preprocessing\n",
        "def text_preprocessing(text):\n",
        "    \n",
        "    prep01 = tokenize_clean(text)\n",
        "    prep02 = remove_stopwords(prep01)\n",
        "    prep03 = stemming_text(prep02)\n",
        "    \n",
        "    return prep03\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynTKVFIIKiUR",
        "colab_type": "text"
      },
      "source": [
        "## Step 01 : Tentukan Set Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubj5bbKuKkwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article = open('sample_data/HCBPC.txt', encoding=\"utf8\").read().split('\\n')\n",
        "len(article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq9TK9zTKqQK",
        "colab_type": "text"
      },
      "source": [
        "## Step 02 : Membentuk Corpus Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yXr_fQmKqxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For gensim we need to tokenize the data and filter out stopwords\n",
        "tokenized_data = []\n",
        "for text in article:\n",
        "    tokenized_data.append(text_preprocessing(text))\n",
        "\n",
        "# Build a Dictionary - association word to numeric id\n",
        "dictionary = corpora.Dictionary(tokenized_data)\n",
        " \n",
        "# Transform the collection of texts to a numerical form\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
        " \n",
        "# Have a look at how the 20th document looks like: [(word_id, count), ...]\n",
        "print(corpus[20])\n",
        "# [(12, 3), (14, 1), (21, 1), (25, 5), (30, 2), (31, 5), (33, 1), (42, 1), (43, 2),  ...\n",
        "print(tokenized_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xikoJsq5LLVV",
        "colab_type": "text"
      },
      "source": [
        "## Step 03 : Membangun Model LDA dan LSI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4B_qwoNL0S2",
        "colab_type": "text"
      },
      "source": [
        "Kita akan coba membangun model LDA and LSI (Latent Semantic Indexing AKA Latent Semantic Analysis) dengan jumlah topik 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieQnm_onLZfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_TOPICS = 4\n",
        "\n",
        "# Membangun LDA model\n",
        "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, alpha = 'auto', eval_every=5)#, per_word_topics=True)\n",
        " \n",
        "# Membangun LSI model\n",
        "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfLdvdOPMI0r",
        "colab_type": "text"
      },
      "source": [
        "Mari kita lihat topik yang telah dihasilkan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL-EnSb-MPHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"LDA Model:\")\n",
        " \n",
        "for idx in range(NUM_TOPICS):\n",
        "    # Print the first 10 most representative topics\n",
        "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
        " \n",
        "print(\"=\" * 20)\n",
        " \n",
        "print(\"LSI Model:\")\n",
        " \n",
        "for idx in range(NUM_TOPICS):\n",
        "    # Print the first 10 most representative topics\n",
        "    print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10))\n",
        " \n",
        "print(\"=\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x1Ytr6fMct_",
        "colab_type": "text"
      },
      "source": [
        "## Step 04 : Transformasi Dokumen Baru"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAGpB-jBMoaf",
        "colab_type": "text"
      },
      "source": [
        "Sekarang kita cobakan model LDA dan LSI ke sebuah dokumen baru."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyZ_k6KHMul_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Selamat ulang tahun pak, semoga sehat dan sukses selalu.\"\n",
        "bow = dictionary.doc2bow(tokenize_and_stem(text))\n",
        "\n",
        "print(lda_model[bow]) \n",
        "print(lsi_model[bow])\n",
        "print(bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5YCDM4nNAEv",
        "colab_type": "text"
      },
      "source": [
        "## Step 05 : Membandingkan dengan Dokumen Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV18zQiANO6j",
        "colab_type": "text"
      },
      "source": [
        "Hasil LDA dapat diinterpretasikan sebagai distribusi terhadap topik. Menggunakan Gensim kita bisa dengan mudah melakukan query terhadap dokumen corpus yang paling mirip dengan dokumen baru."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIQ9XlymNB7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import similarities\n",
        " \n",
        "lda_index = similarities.MatrixSimilarity(lda_model[corpus])\n",
        " \n",
        "# Let's perform some queries\n",
        "similarities = lda_index[lda_model[bow]]\n",
        "# Sort the similarities\n",
        "similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
        " \n",
        "# Top most similar documents:\n",
        "print(similarities[:10])\n",
        " \n",
        "# Let's see what's the most similar document\n",
        "document_id, similarity = similarities[0]\n",
        "print(article[document_id][:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG8M9i4TN0dd",
        "colab_type": "text"
      },
      "source": [
        "## Step 06 : Visualisasi Topik Dokumen Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmXeZlRcN5v3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.gensim\n",
        " \n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "panel"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}